{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Load datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Section 2: Explore the dataset\n",
    "# print(\"Train Data Overview:\")\n",
    "# print(train_data.info())\n",
    "# print(train_data.head())\n",
    "# print(train_data.describe())\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"Missing Values:\")\n",
    "# print(train_data.isnull().sum())\n",
    "\n",
    "# Section 3: Preprocessing\n",
    "# Haversine function to calculate distance between two geo-coordinates\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Combine preprocessing for both train and test data\n",
    "def preprocess_data(data, is_train=True, fraud_counts=None):\n",
    "    if is_train:\n",
    "        # Group by `cc_num` and calculate fraud counts\n",
    "        fraud_counts = data.groupby(['cc_num', 'is_fraud']).size().unstack(fill_value=0).reset_index()\n",
    "        fraud_counts.columns = ['cc_num', 'is_fraud_0_count', 'is_fraud_1_count']\n",
    "        # Add a new column for fraud_score\n",
    "        fraud_counts['fraud_score'] = (fraud_counts['is_fraud_0_count'] * 10) - (fraud_counts['is_fraud_1_count'] * 50)\n",
    "\n",
    "    # Merge fraud counts into the data\n",
    "    data = data.merge(fraud_counts, on='cc_num', how='left')\n",
    "\n",
    "    data['trans_datetime'] = pd.to_datetime(data['trans_date'] + ' ' + data['trans_time'])\n",
    "    data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "    data['age'] = (data['trans_datetime'] - data['dob']).dt.days / 365.25\n",
    "    data['second'] = data['trans_datetime'].dt.hour\n",
    "    data['minute'] = data['trans_datetime'].dt.hour\n",
    "    data['hour'] = data['trans_datetime'].dt.hour\n",
    "    data['day'] = data['trans_datetime'].dt.day\n",
    "    data['month'] = data['trans_datetime'].dt.month\n",
    "    data['weekday'] = data['trans_datetime'].dt.weekday\n",
    "    \n",
    "    data['trans_time_seconds'] = data['trans_datetime'].dt.hour * 3600 + data['trans_datetime'].dt.minute * 60 + data['trans_datetime'].dt.second\n",
    "    data['seconds_from_midnight'] = 43200 - abs(43200 - data['trans_time_seconds'])\n",
    "\n",
    "    # Feature engineering: Calculate distance between cardholder and merchant\n",
    "    data['haversine_distance'] = haversine(\n",
    "        data['lat'], data['long'], data['merch_lat'], data['merch_long']\n",
    "    )\n",
    "\n",
    "    features = [\n",
    "    'amt', 'gender', 'category', 'job', 'state','city_pop',\n",
    "    'hour', 'day', 'month', 'weekday',\n",
    "    'is_fraud_0_count', 'is_fraud_1_count', 'seconds_from_midnight'\n",
    "    'age', 'haversine_distance'\n",
    "    ]\n",
    "    \n",
    "    if(is_train):\n",
    "        features += ['is_fraud']\n",
    "        \n",
    "    data = data[features]\n",
    "\n",
    "    # Convert categorical columns to dummy variables\n",
    "    categorical_cols = ['category', 'state', 'job']\n",
    "    gender_map = {'F': 0, 'M': 1}\n",
    "    data['gender'] = data['gender'].map(gender_map)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        combined_data = pd.concat([data[col]], axis=0).astype(str)\n",
    "        le.fit(combined_data)\n",
    "        data[col] = le.transform(data[col].astype(str))\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data = pd.DataFrame(imputer.fit_transform(data), columns=features)\n",
    "\n",
    "    if is_train:\n",
    "        return data, fraud_counts\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Preprocess train data\n",
    "train_data, fraud_counts = preprocess_data(train_data, is_train=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop('is_fraud', axis=1)\n",
    "y = train_data['is_fraud']\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = preprocess_data(test_data, is_train=False, fraud_counts=fraud_counts)\n",
    "\n",
    "# Ensure the test data has the same columns as training data\n",
    "missing_cols = set(X.columns) - set(test_data.columns)\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[X.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['amt', 'gender', 'category', 'job', 'state', 'city_pop', 'hour', 'day',\n",
      "       'month', 'weekday', 'is_fraud_0_count', 'is_fraud_1_count', 'age',\n",
      "       'haversine_distance', 'is_fraud'],\n",
      "      dtype='object')\n",
      "      amt  gender  category    job  state  city_pop  hour   day  month  \\\n",
      "0  188.38     0.0       9.0    6.0   14.0   95666.0   6.0  10.0    1.0   \n",
      "1  102.63     0.0       4.0  160.0   20.0   37941.0   2.0   6.0    1.0   \n",
      "2    1.62     1.0       0.0   80.0   22.0   19515.0  21.0  18.0    1.0   \n",
      "3    5.64     1.0       5.0  377.0   35.0   62039.0  16.0  21.0    1.0   \n",
      "4   97.09     0.0       5.0  451.0    4.0  106841.0  19.0  21.0    1.0   \n",
      "\n",
      "   weekday  is_fraud_0_count  is_fraud_1_count        age  haversine_distance  \\\n",
      "0      2.0              37.0               0.0  40.626968          104.206730   \n",
      "1      5.0              51.0              10.0  47.091034           60.438265   \n",
      "2      3.0              21.0               3.0  24.402464           86.836599   \n",
      "3      6.0              35.0               1.0  93.905544           97.026084   \n",
      "4      6.0              20.0               9.0  72.268309          108.900685   \n",
      "\n",
      "   is_fraud  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     65840\n",
      "         1.0       0.99      0.97      0.98      8301\n",
      "\n",
      "    accuracy                           1.00     74141\n",
      "   macro avg       0.99      0.99      0.99     74141\n",
      "weighted avg       1.00      1.00      1.00     74141\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65734   106]\n",
      " [  218  8083]]\n",
      "F1 Score: 0.980351728320194\n"
     ]
    }
   ],
   "source": [
    "m2 = model = xgb.XGBClassifier(random_state=4, eval_metric='logloss')\n",
    "m2.fit(X_train, y_train)\n",
    "y2 = m2.predict(X_val)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y2))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y2))\n",
    "\n",
    "print(\"F1 Score:\", f1_score(y_val, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Section 7: Make predictions on the test dataset\n",
    "test_predictions = m2.predict(test_data)\n",
    "\n",
    "# Section 8: Create a submission file\n",
    "submission = pd.DataFrame({'id': sample_submission['id'], 'is_fraud': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
