{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Load datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Section 2: Explore the dataset\n",
    "# print(\"Train Data Overview:\")\n",
    "# print(train_data.info())\n",
    "# print(train_data.head())\n",
    "# print(train_data.describe())\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"Missing Values:\")\n",
    "# print(train_data.isnull().sum())\n",
    "\n",
    "# Section 3: Preprocessing\n",
    "# Haversine function to calculate distance between two geo-coordinates\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Combine preprocessing for both train and test data\n",
    "def preprocess_data(data, is_train=True, fraud_counts=None):\n",
    "    if is_train:\n",
    "        # Group by `cc_num` and calculate fraud counts\n",
    "        fraud_counts = data.groupby(['cc_num', 'is_fraud']).size().unstack(fill_value=0).reset_index()\n",
    "        fraud_counts.columns = ['cc_num', 'is_fraud_0_count', 'is_fraud_1_count']\n",
    "        # Add a new column for fraud_score\n",
    "        fraud_counts['fraud_score'] = (fraud_counts['is_fraud_0_count'] * 10) - (fraud_counts['is_fraud_1_count'] * 50)\n",
    "\n",
    "    # Merge fraud counts into the data\n",
    "    data = data.merge(fraud_counts, on='cc_num', how='left')\n",
    "\n",
    "\n",
    "    # Convert dates to datetime\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date'], errors='coerce')\n",
    "    data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
    "    data['trans_time'] = pd.to_datetime(data['trans_time'], format='%H:%M:%S')\n",
    "\n",
    "    # Feature engineering:\n",
    "    data['age'] = (pd.Timestamp.now() - data['dob']).dt.days // 366\n",
    "    data['trans_hour'] = data['trans_time'].dt.hour\n",
    "    data['trans_minute'] = data['trans_time'].dt.minute\n",
    "    data['trans_second'] = data['trans_time'].dt.second\n",
    "    data['trans_time_seconds'] = data['trans_time'].dt.hour * 3600 + data['trans_time'].dt.minute * 60 + data['trans_time'].dt.second\n",
    "    data['seconds_from_midnight'] = 43200 - abs(43200 - data['trans_time_seconds'])\n",
    "    data['hours_from_midnight'] = 12 - abs(12 - data['trans_hour'])\n",
    "    data['day_of_week'] = data['trans_date'].dt.dayofweek\n",
    "\n",
    "    # Feature engineering: Calculate distance between cardholder and merchant\n",
    "    data['haversine_distance'] = haversine(\n",
    "        data['lat'], data['long'], data['merch_lat'], data['merch_long']\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns, including raw datetime and location fields\n",
    "    columns_to_drop = ['id', 'trans_num', 'cc_num', 'first', 'last', 'street', 'dob', 'trans_date', 'city', 'zip', 'city_pop']\n",
    "    columns_to_drop += ['lat', 'long', 'merch_lat', 'merch_long']\n",
    "    columns_to_drop += ['trans_time', 'trans_hour', 'trans_minute', 'trans_second', 'trans_time_seconds', 'hours_from_midnight']\n",
    "    # columns_to_drop += ['fraud_score', 'is_fraud_0_count', 'is_fraud_1_count']\n",
    "    columns_to_drop += ['fraud_score']\n",
    "    # columns_to_drop += ['category', 'gender', 'state', 'job', 'merchant']\n",
    "    columns_to_drop += ['haversine_distance']\n",
    "    data = data.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "    # Convert categorical columns to dummy variables\n",
    "    categorical_cols = ['category', 'gender', 'state', 'job', 'merchant']\n",
    "    # categorical_cols = ['category', 'gender', 'state', 'job']\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Ensure all remaining columns are numeric\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    if is_train:\n",
    "        return data, fraud_counts\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Preprocess train data\n",
    "train_data, fraud_counts = preprocess_data(train_data, is_train=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop('is_fraud', axis=1)\n",
    "y = train_data['is_fraud']\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = preprocess_data(test_data, is_train=False, fraud_counts=fraud_counts)\n",
    "\n",
    "# Ensure the test data has the same columns as training data\n",
    "missing_cols = set(X.columns) - set(test_data.columns)\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[X.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.columns)\n",
    "print(test_data.columns)\n",
    "# print(train_data.head())\n",
    "\n",
    "# print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeFeature(df, feature):\n",
    "    # Group by 'Score' and calculate the average of the specified feature for each score\n",
    "    avg_feature_by_score = df.groupby('is_fraud')[feature].mean()\n",
    "\n",
    "    print(f\"Average {feature} by Score:\")\n",
    "    print(avg_feature_by_score)\n",
    "\n",
    "    # Correlation between the specified feature and 'Score'\n",
    "    correlation = df[[feature, 'is_fraud']].corr()\n",
    "\n",
    "    print(f\"Correlation between {feature} and Score:\")\n",
    "    print(correlation)\n",
    "    \n",
    "Features = []\n",
    "for ft in Features:\n",
    "  AnalyzeFeature(train_data, ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     65840\n",
      "           1       0.98      0.96      0.97      8301\n",
      "\n",
      "    accuracy                           0.99     74141\n",
      "   macro avg       0.99      0.98      0.98     74141\n",
      "weighted avg       0.99      0.99      0.99     74141\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65713   127]\n",
      " [  336  7965]]\n",
      "F1 Score: 0.9717562374184103\n"
     ]
    }
   ],
   "source": [
    "m2 = model = xgb.XGBClassifier(eval_metric='logloss', random_state=0)\n",
    "m2.fit(X_train, y_train)\n",
    "y2 = m2.predict(X_val)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y2))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y2))\n",
    "\n",
    "print(\"F1 Score:\", f1_score(y_val, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Make predictions on the test dataset\n",
    "test_predictions = m2.predict(test_data)\n",
    "\n",
    "# Section 8: Create a submission file\n",
    "submission = pd.DataFrame({'id': sample_submission['id'], 'is_fraud': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
